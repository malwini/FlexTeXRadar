name,ring,quadrant,isNew,description
AWS MWAA,Adopt,Techniques,FALSE,"<p><strong><a href=""https://arxiv.org/abs/2005.11401v4"">Retrieval-augmented generation (RAG)</a></strong> is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular <a href=""https://www.jugalbandi.ai/mission"">Jugalbandi AI Platform</a>. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as <a href=""/radar/platforms/pgvector"">pgvector</a>, <a href=""/radar/platforms/qdrant"">Qdrant</a> or <a href=""/radar/platforms/elasticsearch-relevance-engine"">Elasticsearch Relevance Engine</a>. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.</p>"